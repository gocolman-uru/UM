{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "<h1 align=\"center\"> Trabajo Obligatorio - Pensamiento Computacional - 2023 </h1>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "   <img src=\"https://github.com/gocolman-uru/UM/blob/obligatorio_pens_computacional/test/static/assets/img/ApUMtes.jpg\" width=\"75\" alt=\"apuntes\">\n",
    "</p>\n",
    "\n",
    "* Status: accepted \n",
    "* Deciders: Gonzalo Colman\n",
    "* Date: 2022-08-01\n",
    "\n",
    "Para realizar la ejecuciÃ³n del programa, dejamos el detalle del [Manual de usuario] con sus pasos a seguir. (https://github.com/gocolman-uru/UM/blob/obligatorio_pens_computacional/Manual_de_usuario.md)\n",
    "\n",
    "Technical Story: Given an rep's comment when the case has been closed, we provide the key tags, risk features with their score, that has been descripted in the comment block. \n",
    "\n",
    "\n",
    "## Model Overview\n",
    "* Problem Family: supervised\n",
    "* Model Family: POC clustering ---> Multitag\n",
    "* Model Type: Ngrams & RandomForestClassifier (OneVsRestClassifier)\n",
    "* Model setup: \n",
    "                              - class_weight = 'balanced',\n",
    "                              - n_estimators = 200, \n",
    "                              - max_features = log2,\n",
    "                              - max_depth = 10,\n",
    "                              - criterion = 'gini'\n",
    "                              \n",
    "                              \n",
    "* Model Dimension:  \n",
    "\n",
    "                    def train_model(name_transformer, X, y , estimador):\n",
    "                        vectores = encoder_transformer(name_transformer,X)\n",
    "                        clf = estimador.fit(vectores, y)\n",
    "                        return clf\n",
    "                        \n",
    "                        El modelo utiliza SentenceTransformer --> 'multi-qa-mpnet-base-dot-v1'\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "* Dataset Dimension: **(10346, 23)**\n",
    "* Test size: **0.15** (1342,23)\n",
    "* Validation size: **0.15** (1342,23)\n",
    "\n",
    "* Validation Metric: precision\n",
    "* Validation Performance: **0.883** --> **general** <font size = 1>(*Notebook 2_model_exploration.ipynb*)</font>\n",
    "\n",
    "# Dataset\n",
    "## Classes (subtypes)\n",
    "\n",
    "In order to achieve the subtype's prediction, first we had to define the correct classes and the definition of what comment is associated to the right casuistry.\n",
    "\n",
    "We took all the subtypes of fraud prevention, and unified in some ways that in the end we would have only few general classes, grouped by a sort of \"theme\". For example, we unified bunch of payments cases that corresponse to differents subtypes in one, so we can assert a payment's class in the trainset without adding granularity. \n",
    "\n",
    "* Label Version [1.0.0]\n",
    "* Label date-range: 2022-01-01 to 2022-09-01\n",
    "* Label cardinality: 9 possible classes. Label encoder field 'gca_subtype_encoder'\n",
    "\n",
    "> #### Target dataset\n",
    "\n",
    "\n",
    "                    \n",
    "                    \n",
    "> After all, we applied some filters and transformations\n",
    "\n",
    "                - Exclude null comments\n",
    "                - Join EV and Restriction fields\n",
    "                - Only comments with len() > 10\n",
    "                - Exclude cases with multiple subtypes\n",
    "                - Exclude RM payment cases\n",
    "                - Exclude EV = (DOK|DM) with null Restriction\n",
    "                - Exclude Restriction = (CBK) with another EV rather than Prev_inhab_perm \n",
    "                - Classes unified (details in settings.py)\n",
    "                - Exclude EV = POI & Otros\n",
    "                - Exclude EV = (DM) and Restriction = Fraude_ml\n",
    "                - Exclude EV = None and Restriction = Ret_mp\n",
    "                - Only consider cases that had been closed with Deactive resolution\n",
    "                - Exclude generic comments (details in settings.py)\n",
    "                - Exclude two script comments (details in settings.py)\n",
    "                - Exclude Bacen and CC cases\n",
    "    \n",
    "        \n",
    "\n",
    "## Considerations\n",
    "We sample the trainset and balanced our Classes with 100k records per class. \n",
    "\n",
    "## Feature Summary\n",
    "* Number of features: 128 word embeddings\n",
    "* Name of target column: **gca_subtype_encoder**\n",
    "\n",
    "\n",
    "## Text pre-processing\n",
    "\n",
    "* ETL stage: [None]\n",
    "* Model building stage: [Text to lower, remove accents, remove punctuation, Join comments EV+Restriction]\n",
    "\n",
    "\n",
    "\n",
    "## Context and Problem Statement\n",
    "\n",
    "This kind of situation happens in the Manual Review universe. It's a matter of time that automatic decisions will scale and PF's operation teams lean on strong rules and model outputs more than yesterday. \n",
    "When an user's contact incomes in the manual review queue, a set of strong rules (based on multiple features) classified and redirects the case into the right team. At the bottom of this \"if sentence\", there is always an \"else\". There is where the problem lives. Fraud prevention's Representants are assigned to this task to analyze and manually classified that cases.\n",
    "\n",
    "## Decision Drivers \n",
    "\n",
    "We know since the beginning of the project that it would be an NLP problem. Our input are the EV and Restriction's comment, so we ended to consider the AutoKeras text classifier. We walked this journey as a one team, from the analytics squad to the data scientists involved in this project. Weekly huddles and slack channels were crucial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
